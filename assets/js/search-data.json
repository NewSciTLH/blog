{
  
    
        "post0": {
            "title": "Deploying Custom Models in Vertex AI",
            "content": "!mkdir predictor !mkdir predictor/model . Within the model directory, place all of your model artifacts. Your model artifacts are a list of files needed to perform inference on your model. This will range from .bin file storing the weights of your model, to config files, to a tokenizer, all models will be different. But simply put, whatever files you need to perform inference on your model, put them in a predictor/model directory. . Step 3 Preparing to Dockerize TorchServe . To host any model in Vertex AI, your docker image needs to contain instructions to run a HTTP server that can serve as a means of retrieving inferences and checking the health of the server. There are many ways to do this, such as making a flask app along with most major ML frameworks containing some kind of their own serving software. TorchServe is the one for PyTorch models. . TorchServe requires what is called a handler. A handler is a python class that handles all the pre/post processing of inputs/outputs as well as the actual code of loading your model, tokenizer, whatever you need for performing inference on your model. TorchServe has many prebuilt handlers found here. Here is my own custom handler. . import os import json import logging from os import listdir import torch from transformers import AutoTokenizer, OPTForCausalLM from ts.torch_handler.base_handler import BaseHandler logger = logging.getLogger(__name__) class TransformersHandler(BaseHandler): &quot;&quot;&quot; This handler takes in a input string and multiple parameters and returns autoregressive generations from various OPT models. &quot;&quot;&quot; def __init__(self): super(TransformersHandler, self).__init__() self.initialized = False def initialize(self, ctx): &quot;&quot;&quot; The function looks at the specs of the device that is running the server and loads in the model and any other objects that must be loaded in. &quot;&quot;&quot; # get the passed properties of the torchserve compiler and the device self.manifest = ctx.manifest properties = ctx.system_properties model_dir = properties.get(&quot;model_dir&quot;) self.device = torch.device(&quot;cuda:&quot; + str(properties.get(&quot;gpu_id&quot;)) if torch.cuda.is_available() else &quot;cpu&quot;) # Read model serialize/pt file serialized_file = self.manifest[&quot;model&quot;][&quot;serializedFile&quot;] model_pt_path = os.path.join(model_dir, serialized_file) if not os.path.isfile(model_pt_path): raise RuntimeError(&quot;Missing the model.pt or pytorchf_model.bin file&quot;) # Load model logger.info(&quot;Loading Model...&quot;) self.model = OPTForCausalLM.from_pretrained(model_dir) logger.info(&quot;Model loaded...&quot;) self.model.to(self.device) logger.debug(&#39;Transformer model from path {0} loaded successfully&#39;.format(model_dir)) # Ensure to use the same tokenizer used during training logger.info(&quot;Loading tokenizer...&quot;) self.tokenizer = AutoTokenizer.from_pretrained(model_dir) logger.info(&quot;Tokenizer loaded&quot;) self.initialized = True def preprocess(self, data): &quot;&quot;&quot; The initial entry of data being passed for inference. Here it is where we extract the parameters and inputs. Inputs are tokenized for inference. &quot;&quot;&quot; params = data[0].get(&quot;parameters&quot;) text = data[0].get(&quot;data&quot;).get(&#39;text&#39;) # set the params self.num_return_sequences = params.get(&#39;num_return_sequences&#39;) self.top_p = params.get(&#39;top_p&#39;) self.top_k = params.get(&#39;top_k&#39;) self.temperature = params.get(&#39;temperature&#39;) self.max_length = params.get(&#39;max_length&#39;) self.no_repeat_ngram_size = params.get(&#39;no_repeat_ngram_size&#39;) inputs = self.tokenizer(text, return_tensors=&#39;pt&#39;) return inputs def inference(self, inputs): &quot;&quot;&quot; Function for performing inference on the processed input. The predictions are then decoded and returned. &quot;&quot;&quot; prediction = self.model.generate(inputs.input_ids, max_length=self.max_length, num_return_sequences= self.num_return_sequences, do_sample = True, temperature = self.temperature, early_stopping = True, top_k = self.top_k, top_p = self.top_p, no_repeat_ngram_size = 2, return_dict_in_generate = True, tokenizer = self.tokenizer) prediction = self.tokenizer.batch_decode(prediction[&#39;sequences&#39;], skip_special_tokens=True, clean_up_tokenization_spaces=False) return [prediction] def postprocess(self, inference_output): &#39;&#39;&#39; Extra function for processing inference outputs if not already done so. &#39;&#39;&#39; return inference_output . This is a custom handler for inference on an autoregressive language model from huggingface, in this case I am using OPT-125m. You can see that the handler is made up of different pieces such as initializing the model for inference, processing inputs and outputs. Whatever your handler is, it will have to match the specifics of the model you want to perform inference on and what pre/post processing are involved with the inputs/outputs of said model. . Place your handler in the predictor directory. . Step 4 Define Statics . We need to assign some variables that pertain to your specific project. Project ID should be your GCP project id, app name should be whatever you want your app to be called. . PROJECT_ID = &#39;my-fun-project&#39; APP_NAME = &quot;vertex-test&quot; CUSTOM_PREDICTOR_IMAGE_URI = f&quot;gcr.io/{PROJECT_ID}/pytorch_predict_{APP_NAME}&quot; . Step 5 Write Your Dockerfile . docker_file = &#39;&#39;&#39; bash -s $APP_NAME APP_NAME=$1 cat &lt;&lt; EOF &gt; ./predictor/Dockerfile FROM pytorch/torchserve:latest-cpu # install dependencies RUN python3 -m pip install --upgrade pip RUN pip3 install transformers RUN pip3 install torch USER model-server # copy model artifacts, custom handler and other dependencies COPY {0} /home/model-server/ COPY ./model/ /home/model-server/ # create torchserve configuration files USER root RUN printf &quot; nservice_envelope=json&quot; &gt;&gt; /home/model-server/config.properties RUN printf &quot; ninference_address=http://0.0.0.0:7080&quot; &gt;&gt; /home/model-server/config.properties RUN printf &quot; nmanagement_address=http://0.0.0.0:7081&quot; &gt;&gt; /home/model-server/config.properties USER model-server # expose health and prediction listener ports from the image EXPOSE 7080 EXPOSE 7081 # create model archive file packaging model artifacts and dependencies RUN torch-model-archiver -f --model-name={1} --version={7} --serialized-file=/home/model-server/{4} --handler=/home/model-server/{5} --extra-files &quot;{6}&quot; --export-path=/home/model-server/model-store # run Torchserve HTTP serve to respond to prediction requests CMD [&quot;torchserve&quot;, &quot;--start&quot;, &quot;--ts-config=/home/model-server/config.properties&quot;, &quot;--models&quot;, &quot;{2}={3}.mar&quot;, &quot;--model-store&quot;, &quot;/home/model-server/model-store&quot;] EOF echo &quot;Writing ./predictor/Dockerfile&quot; &#39;&#39;&#39;.format(handler_name,APP_NAME,APP_NAME,APP_NAME,model_source.split(&#39;/&#39;)[model_source.count(&#39;/&#39;)],handler_name,&#39;,&#39;.join(extra_files),str(VERSION)) #write docker file os.system(docker_file) . The Dockerfile must be within your predictor directory. . Step 6 Build and Push Image to GCP . Now that we have our Dockerfile all set, the next thing to do is the use docker to build and deploy an image in GCP. To do this, we need to determine what our image URI will be in GCP and build the Dockerfile to the image. Docker will take the Dockerfile and the artifacts for our model/software that are all in the predictor directory, and build it into an image. When the image is built, we then want to deploy it into the GCP registry. . CUSTOM_PREDICTOR_IMAGE_URI = f&quot;gcr.io/{PROJECT_ID}/pytorch_predict_{APP_NAME}&quot; # build the image !docker build --tag=$CUSTOM_PREDICTOR_IMAGE_URI ./predictor # deploy the image to the GCP registry !docker push $CUSTOM_PREDICTOR_IMAGE_URI . Step 7 Docker Image to Vertex AI Model . Normally, docker images are deployed into a container, or a runnable instance of that docker image. To move past Vertex nomenclature, we can view vertex AI endpoints as containers and models as images. We are simply taking an image in the GCP container registry and moving it into Vertex AI as a &quot;model&quot;. We will do this with the GCP python SDK. . from google.cloud import aiplatform aiplatform.init(project=PROJECT_ID) VERSION = 1 model_display_name = f&quot;{APP_NAME}-v{VERSION}&quot; model_description = &quot;This is so fun&quot; MODEL_NAME = APP_NAME health_route = &quot;/ping&quot; predict_route = f&quot;/predictions/{MODEL_NAME}&quot; serving_container_ports = [7080] . model = aiplatform.Model.upload( display_name=model_display_name, description=model_description, serving_container_image_uri=CUSTOM_PREDICTOR_IMAGE_URI, serving_container_predict_route=predict_route, serving_container_health_route=health_route, serving_container_ports=serving_container_ports, ) model.wait() print(model.display_name) print(model.resource_name) . Step 8 Containerizing your Vertex Model . Now that you have deployed your image/model in vertex AI, you now need to containerize it, or give it a home within a runnable instance. In Vertex AI, these are called endpoints. An endpoint is what assigns the machine to run your callable http server to perform inferences and health checks. . endpoint_display_name = f&quot;{APP_NAME}-endpoint&quot; endpoint = aiplatform.Endpoint.create(display_name=endpoint_display_name) traffic_percentage = 100 # If you have multiple models assigned to a single endpoint, you can split traffic between the models. machine_type = &quot;n1-standard-16&quot; # assign what type of machine you want in GCP, list found here https://cloud.google.com/vertex-ai/docs/predictions/configure-compute deployed_model_display_name = model_display_name sync = True # deploy your model, to your endpoint. This may take some time. model.deploy( endpoint=endpoint, deployed_model_display_name=deployed_model_display_name, machine_type=machine_type, traffic_percentage=traffic_percentage, sync=sync, ) . When your model has been deployed to your endpoint, you will get a message of something like &quot;projects/{myprojectID}/locations/us-central1/endpoints/{endpoint ID}&quot;. This is what is known as the &quot;resource name&quot; and you will need it to perform inferences. . Step 9 Performing Inference . To perform inference, make an endpoint object and pass the endpoint resource name as the only argument. Format json-like data to pass to your endpoint object that coincides with whatever your handler takes in. For my handler, I define variable t as my input for inference. . endpoint = aiplatform.Endpoint(&#39;{my endpoint resource}&#39;) endpoint.list_models() t = [ { &quot;data&quot;: { &quot;text&quot;: &quot;This is a test&quot; }, &quot;parameters&quot;:{ &quot;num_return_sequences&quot;: 5, &quot;top_p&quot;:0.9, &quot;top_k&quot;:10, &quot;temperature&quot;:0.8, &quot;max_length&quot;:20, &quot;no_repeat_ngram_size&quot;:2 } } ] # call the prediction prediction = endpoint.predict(instances=t) . Conclusion . And there you have it, you will have your model&#39;s prediction. In this tutorial I walk through how to deploy custom PyTorch models in Vertex AI. To continue your educational journey, play around with what you produced, make it your own, and read up on the many services used here. I hope you found it useful and I am happy to connect with anyone via linkedin. . All the code and the deployment of PyTorch models have been packaged in a public software I wrote which you can find here. https://github.com/jamesonhohbein/vertex_deployment_4dummies . Warm regards, . Jameson .",
            "url": "https://newscitlh.github.io/blog/2022/07/18/_07_19_Deploying_Custom_Models_in_Vertex_AI.html",
            "relUrl": "/2022/07/18/_07_19_Deploying_Custom_Models_in_Vertex_AI.html",
            "date": " • Jul 18, 2022"
        }
        
    
  

  
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://newscitlh.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}