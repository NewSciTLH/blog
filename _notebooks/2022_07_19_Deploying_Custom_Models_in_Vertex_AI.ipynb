{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2022-07-19-Deploying-Custom-Models-in-Vertex-AI.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Deploying Custom Models in Vertex AI \n",
        "By Jameson Hohbein\n",
        "[Linkedin](https://www.linkedin.com/in/jameson-hohbein/) [Github](https://github.com/jamesonhohbein)\n",
        "\n",
        "\n",
        "*   toc: true\n",
        "*   badges: true\n",
        "*   comments: true\n",
        "*   categories: [jupyter]\n",
        "*   image: images/vertex.png\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ay8f3IRydGka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "This tutorial is for people who want to deploy a functioning custom PyTorch model in Vertex AI. This tutorial is not a substitute for reading the documentation of Vertex AI, TorchServe, PyTorch, or any other service used in this tutorial. This is only for people who want to learn and get an introduction through hands-on, project-based learning. \n",
        "\n",
        "## Background \n",
        "Vertex AI has now taken over as Google Cloud Platform's center for all things pertaining to AI. As far as I have seen, there are little to no tutorials going in to deploying custom models in vertex AI. In this tutorial, I am going to define each step to host your own custom PyTorch models in Vertex AI since Vertex AI does not have any prebuilt containers for deploying PyTorch models. You will be able to scale and customize the software around your model as you see fit. \n",
        "\n",
        "\n",
        "## Step 1 Select Your Environment \n",
        "We will be working with docker images and google cloud SDK. I personally recommend spinning up a jupyter notebook instance in vertex AI workbench, or running your own jupyter instance on your local machine. Docker is incompatible with google colab and other high level notebook services so you will need a dedicated instance that can run docker. \n",
        "\n",
        "## Step 2 Collect your Artifacts \n",
        "Now that you have your dedicated work environment, you will need to collect your model artifacts into whatever environment you are working in. First thing is first, let's organize our files. Create a new directory titled 'predictor' and within that directory, another one called, 'model'.\n"
      ],
      "metadata": {
        "id": "xWFlJLbgz60i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir predictor\n",
        "!mkdir predictor/model"
      ],
      "metadata": {
        "id": "ywPTQtVth7Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Within the model directory, place all of your model artifacts. Your model artifacts are a list of files needed to perform inference on your model. This will range from .bin file storing the weights of your model, to config files, to a tokenizer, all models will be different. But simply put, whatever files you need to perform inference on your model, put them in a predictor/model directory. \n",
        "\n",
        "## Step 3 Preparing to Dockerize TorchServe\n",
        "To host any model in Vertex AI, your docker image needs to contain instructions to run a HTTP server that can serve as a means of retrieving inferences and checking the health of the server. There are many ways to do this, such as making a flask app along with most major ML frameworks containing some kind of their own serving software. TorchServe is the one for PyTorch models. \n",
        "\n",
        "TorchServe requires what is called a handler. A handler is a python class that handles all the pre/post processing of inputs/outputs as well as the actual code of loading your model, tokenizer, whatever you need for performing inference on your model. TorchServe has many prebuilt handlers [found here](https://pytorch.org/serve/default_handlers.html). Here is my own custom handler. "
      ],
      "metadata": {
        "id": "0hzaou8gh8nJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "from os import listdir\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, OPTForCausalLM\n",
        "from ts.torch_handler.base_handler import BaseHandler\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class TransformersHandler(BaseHandler):\n",
        "    \"\"\"\n",
        "    This handler takes in a input string and multiple parameters and returns autoregressive generations from various OPT models. \n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(TransformersHandler, self).__init__()\n",
        "        self.initialized = False\n",
        "\n",
        "    def initialize(self, ctx):\n",
        "        \"\"\" \n",
        "        The function looks at the specs of the device that is running the server and loads in the model and any other objects that must be loaded in.\n",
        "        \n",
        "        \"\"\"\n",
        "        # get the passed properties of the torchserve compiler and the device \n",
        "        self.manifest = ctx.manifest\n",
        "        properties = ctx.system_properties\n",
        "        model_dir = properties.get(\"model_dir\")\n",
        "        self.device = torch.device(\"cuda:\" + str(properties.get(\"gpu_id\")) if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Read model serialize/pt file\n",
        "        serialized_file = self.manifest[\"model\"][\"serializedFile\"]\n",
        "        \n",
        "        model_pt_path = os.path.join(model_dir, serialized_file)\n",
        "        if not os.path.isfile(model_pt_path):\n",
        "            raise RuntimeError(\"Missing the model.pt or pytorchf_model.bin file\")\n",
        "        \n",
        "        # Load model\n",
        "        logger.info(\"Loading Model...\")\n",
        "        self.model = OPTForCausalLM.from_pretrained(model_dir)\n",
        "        logger.info(\"Model loaded...\")\n",
        "        \n",
        "        self.model.to(self.device)\n",
        "        \n",
        "        logger.debug('Transformer model from path {0} loaded successfully'.format(model_dir))\n",
        "        \n",
        "        # Ensure to use the same tokenizer used during training\n",
        "        logger.info(\"Loading tokenizer...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "        logger.info(\"Tokenizer loaded\")\n",
        "\n",
        "        self.initialized = True\n",
        "\n",
        "    def preprocess(self, data):\n",
        "        \"\"\"\n",
        "        The initial entry of data being passed for inference. \n",
        "        Here it is where we extract the parameters and inputs. \n",
        "        Inputs are tokenized for inference.\n",
        "        \"\"\"\n",
        "        params = data[0].get(\"parameters\")\n",
        "        text = data[0].get(\"data\").get('text')\n",
        "        \n",
        "        # set the params \n",
        "        self.num_return_sequences = params.get('num_return_sequences')\n",
        "        self.top_p = params.get('top_p')\n",
        "        self.top_k = params.get('top_k')\n",
        "        self.temperature = params.get('temperature')\n",
        "        self.max_length = params.get('max_length')\n",
        "        self.no_repeat_ngram_size = params.get('no_repeat_ngram_size')\n",
        "        \n",
        "        inputs = self.tokenizer(text, return_tensors='pt')\n",
        "\n",
        "        return inputs\n",
        "\n",
        "    def inference(self, inputs):\n",
        "        \"\"\"\n",
        "        Function for performing inference on the processed input. The predictions are then decoded and returned.\n",
        "        \"\"\"\n",
        "        \n",
        "        prediction = self.model.generate(inputs.input_ids,\n",
        "                                         max_length=self.max_length,\n",
        "                                         num_return_sequences= self.num_return_sequences,\n",
        "                                         do_sample = True,\n",
        "                                         temperature = self.temperature,\n",
        "                                         early_stopping = True,\n",
        "                                         top_k = self.top_k,\n",
        "                                         top_p = self.top_p,\n",
        "                                         no_repeat_ngram_size = 2,\n",
        "                                         return_dict_in_generate = True,\n",
        "                                         tokenizer = self.tokenizer)\n",
        "        \n",
        "        \n",
        "        prediction = self.tokenizer.batch_decode(prediction['sequences'],\n",
        "                                                  skip_special_tokens=True,\n",
        "                                                  clean_up_tokenization_spaces=False)\n",
        "        \n",
        "        return [prediction]\n",
        "\n",
        "    def postprocess(self, inference_output):\n",
        "        '''\n",
        "        Extra function for processing inference outputs if not already done so.\n",
        "        '''\n",
        "        return inference_output\n"
      ],
      "metadata": {
        "id": "X9aW-bUCjp9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a custom handler for inference on an autoregressive language model from huggingface, in this case I am using OPT-125m. You can see that the handler is made up of different pieces such as initializing the model for inference, processing inputs and outputs. Whatever your handler is, it will have to match the specifics of the model you want to perform inference on and what pre/post processing are involved with the inputs/outputs of said model. \n",
        "\n",
        "Place your handler in the predictor directory.\n",
        "\n",
        "\n",
        "## Step 4 Define Statics\n",
        "\n",
        "We need to assign some variables that pertain to your specific project. Project ID should be your GCP project id, app name should be whatever you want your app to be called. \n"
      ],
      "metadata": {
        "id": "JWfGiwk1jxjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = 'my-fun-project'\n",
        "APP_NAME = \"vertex-test\"\n",
        "CUSTOM_PREDICTOR_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/pytorch_predict_{APP_NAME}\""
      ],
      "metadata": {
        "id": "HGDCcYSKlVvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5 Write Your Dockerfile"
      ],
      "metadata": {
        "id": "-28I72ZLmsvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docker_file = '''   \n",
        "bash -s $APP_NAME\n",
        "\n",
        "APP_NAME=$1\n",
        "\n",
        "cat << EOF > ./predictor/Dockerfile\n",
        "\n",
        "FROM pytorch/torchserve:latest-cpu\n",
        "\n",
        "# install dependencies\n",
        "RUN python3 -m pip install --upgrade pip\n",
        "RUN pip3 install transformers\n",
        "RUN pip3 install torch\n",
        "\n",
        "USER model-server\n",
        "\n",
        "# copy model artifacts, custom handler and other dependencies\n",
        "COPY {0} /home/model-server/\n",
        "COPY ./model/ /home/model-server/\n",
        "\n",
        "# create torchserve configuration files\n",
        "USER root\n",
        "RUN printf \"\\\\nservice_envelope=json\" >> /home/model-server/config.properties\n",
        "RUN printf \"\\\\ninference_address=http://0.0.0.0:7080\" >> /home/model-server/config.properties\n",
        "RUN printf \"\\\\nmanagement_address=http://0.0.0.0:7081\" >> /home/model-server/config.properties\n",
        "USER model-server\n",
        "\n",
        "# expose health and prediction listener ports from the image\n",
        "EXPOSE 7080\n",
        "EXPOSE 7081\n",
        "\n",
        "# create model archive file packaging model artifacts and dependencies\n",
        "RUN torch-model-archiver -f   --model-name={1}   --version={7}   --serialized-file=/home/model-server/{4}   --handler=/home/model-server/{5}   --extra-files \"{6}\"   --export-path=/home/model-server/model-store\n",
        "\n",
        "# run Torchserve HTTP serve to respond to prediction requests\n",
        "CMD [\"torchserve\",      \"--start\",      \"--ts-config=/home/model-server/config.properties\",      \"--models\",      \"{2}={3}.mar\",      \"--model-store\",      \"/home/model-server/model-store\"]\n",
        "\n",
        "EOF\n",
        "\n",
        "echo \"Writing ./predictor/Dockerfile\"\n",
        "'''.format(handler_name,APP_NAME,APP_NAME,APP_NAME,model_source.split('/')[model_source.count('/')],handler_name,','.join(extra_files),str(VERSION))\n",
        "\n",
        "#write docker file\n",
        "os.system(docker_file)"
      ],
      "metadata": {
        "id": "ExedLz0fjYfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Dockerfile must be within your predictor directory. "
      ],
      "metadata": {
        "id": "miLHvFCyb8ST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6 Build and Push Image to GCP\n",
        "\n",
        "Now that we have our Dockerfile all set, the next thing to do is the use docker to build and deploy an image in GCP. To do this, we need to determine what our image URI will be in GCP and build the Dockerfile to the image. Docker will take the Dockerfile and the artifacts for our model/software that are all in the predictor directory, and build it into an image. When the image is built, we then want to deploy it into the GCP registry. "
      ],
      "metadata": {
        "id": "lq0eUXQcesXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the URI\n",
        "CUSTOM_PREDICTOR_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/pytorch_predict_{APP_NAME}\"\n",
        "\n",
        "# build the image\n",
        "!docker build --tag=$CUSTOM_PREDICTOR_IMAGE_URI ./predictor\n",
        "\n",
        "# deploy the image to the GCP registry\n",
        "!docker push $CUSTOM_PREDICTOR_IMAGE_URI"
      ],
      "metadata": {
        "id": "fq658zTofFQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7 Docker Image to Vertex AI Model\n",
        "\n",
        "Normally, docker images are deployed into a container, or a runnable instance of that docker image. To move past Vertex nomenclature, we can view vertex AI endpoints as containers and models as images. We are simply taking an image in the GCP container registry and moving it into Vertex AI as a \"model\". We will do this with the GCP python SDK. "
      ],
      "metadata": {
        "id": "QxyYiRZ6fSY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the google-cloud-aiplatform package and assign some static variables \n",
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID)\n",
        "VERSION = 1\n",
        "model_display_name = f\"{APP_NAME}-v{VERSION}\"\n",
        "model_description = \"This is so fun\"\n",
        "\n",
        "MODEL_NAME = APP_NAME\n",
        "health_route = \"/ping\"\n",
        "predict_route = f\"/predictions/{MODEL_NAME}\"\n",
        "serving_container_ports = [7080]\n"
      ],
      "metadata": {
        "id": "IUmWJSJugp9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now deploy the model into Vertex AI\n",
        "model = aiplatform.Model.upload(\n",
        "    display_name=model_display_name,\n",
        "    description=model_description,\n",
        "    serving_container_image_uri=CUSTOM_PREDICTOR_IMAGE_URI,\n",
        "    serving_container_predict_route=predict_route,\n",
        "    serving_container_health_route=health_route,\n",
        "    serving_container_ports=serving_container_ports,\n",
        ")\n",
        "\n",
        "model.wait()\n",
        "\n",
        "print(model.display_name)\n",
        "print(model.resource_name)"
      ],
      "metadata": {
        "id": "GRCqvWgxg9x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8 Containerizing your Vertex Model\n",
        "\n",
        "Now that you have deployed your image/model in vertex AI, you now need to containerize it, or give it a home within a runnable instance. In Vertex AI, these are called endpoints. An endpoint is what assigns the machine to run your callable http server to perform inferences and health checks. "
      ],
      "metadata": {
        "id": "XvVQNjv1ho4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the endpoint here. Initially it will just be an empty object registered in Vertex AI\n",
        "endpoint_display_name = f\"{APP_NAME}-endpoint\"\n",
        "endpoint = aiplatform.Endpoint.create(display_name=endpoint_display_name)\n",
        "\n",
        "\n",
        "traffic_percentage = 100 # If you have multiple models assigned to a single endpoint, you can split traffic between the models. \n",
        "machine_type = \"n1-standard-16\" # assign what type of machine you want in GCP, list found here https://cloud.google.com/vertex-ai/docs/predictions/configure-compute\n",
        "deployed_model_display_name = model_display_name\n",
        "sync = True\n",
        "# deploy your model, to your endpoint. This may take some time. \n",
        "model.deploy(\n",
        "    endpoint=endpoint,\n",
        "    deployed_model_display_name=deployed_model_display_name,\n",
        "    machine_type=machine_type,\n",
        "    traffic_percentage=traffic_percentage,\n",
        "    sync=sync,\n",
        ")"
      ],
      "metadata": {
        "id": "vvRPYqhViL3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When your model has been deployed to your endpoint, you will get a message of something like \"projects/{myprojectID}/locations/us-central1/endpoints/{endpoint ID}\". This is what is known as the \"resource name\" and you will need it to perform inferences. "
      ],
      "metadata": {
        "id": "IQVymY0ZirzL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9 Performing Inference \n",
        "\n",
        "To perform inference, make an endpoint object and pass the endpoint resource name as the only argument. Format json-like data to pass to your endpoint object that coincides with whatever your handler takes in. For my handler, I define variable t as my input for inference. "
      ],
      "metadata": {
        "id": "cOQCXGZUi-aL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint = aiplatform.Endpoint('{my endpoint resource}')\n",
        "endpoint.list_models()\n",
        "\n",
        "t = [\n",
        "    {\n",
        "        \"data\": {\n",
        "            \"text\": \"This is a test\"\n",
        "        },\n",
        "        \"parameters\":{\n",
        "            \"num_return_sequences\": 5,\n",
        "            \"top_p\":0.9,\n",
        "            \"top_k\":10,\n",
        "            \"temperature\":0.8,\n",
        "            \"max_length\":20,\n",
        "            \"no_repeat_ngram_size\":2\n",
        "            \n",
        "            \n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "# call the prediction \n",
        "prediction = endpoint.predict(instances=t)"
      ],
      "metadata": {
        "id": "RB2jk3dpZnPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion \n",
        "\n",
        "And there you have it, you will have your model's prediction. In this tutorial I walk through how to deploy custom PyTorch models in Vertex AI. To continue your educational journey, play around with what you produced, make it your own, and read up on the many services used here. I hope you found it useful and I am happy to connect with anyone via [linkedin](https://www.linkedin.com/in/jameson-hohbein/). \n",
        "\n",
        "All the code and the deployment of PyTorch models have been packaged in a public software I wrote which you can find here. \n",
        "https://github.com/jamesonhohbein/vertex_deployment_4dummies\n",
        "\n",
        "\n",
        "Warm regards,\n",
        "\n",
        "Jameson\n"
      ],
      "metadata": {
        "id": "i2G8Vic_byqi"
      }
    }
  ]
}