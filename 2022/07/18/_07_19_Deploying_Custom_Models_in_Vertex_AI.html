<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Deploying Custom Models in Vertex AI | Call Simulator Blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Deploying Custom Models in Vertex AI" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A blog for Call Simulator, a technology company. https://callsimulator.com/" />
<meta property="og:description" content="A blog for Call Simulator, a technology company. https://callsimulator.com/" />
<link rel="canonical" href="https://newscitlh.github.io/blog/2022/07/18/_07_19_Deploying_Custom_Models_in_Vertex_AI.html" />
<meta property="og:url" content="https://newscitlh.github.io/blog/2022/07/18/_07_19_Deploying_Custom_Models_in_Vertex_AI.html" />
<meta property="og:site_name" content="Call Simulator Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-07-18T00:00:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Deploying Custom Models in Vertex AI" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-07-18T00:00:00-05:00","datePublished":"2022-07-18T00:00:00-05:00","description":"A blog for Call Simulator, a technology company. https://callsimulator.com/","headline":"Deploying Custom Models in Vertex AI","mainEntityOfPage":{"@type":"WebPage","@id":"https://newscitlh.github.io/blog/2022/07/18/_07_19_Deploying_Custom_Models_in_Vertex_AI.html"},"url":"https://newscitlh.github.io/blog/2022/07/18/_07_19_Deploying_Custom_Models_in_Vertex_AI.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://newscitlh.github.io/blog/feed.xml" title="Call Simulator Blog" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Call Simulator Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Deploying Custom Models in Vertex AI</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-07-18T00:00:00-05:00" itemprop="datePublished">
        Jul 18, 2022
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      9 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/NewSciTLH/blog/tree/master/_notebooks/2022_07_19_Deploying_Custom_Models_in_Vertex_AI.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/NewSciTLH/blog/master?filepath=_notebooks%2F2022_07_19_Deploying_Custom_Models_in_Vertex_AI.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/NewSciTLH/blog/blob/master/_notebooks/2022_07_19_Deploying_Custom_Models_in_Vertex_AI.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
          <div class="px-2">
  <a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2FNewSciTLH%2Fblog%2Fblob%2Fmaster%2F_notebooks%2F2022_07_19_Deploying_Custom_Models_in_Vertex_AI.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/blog/assets/badges/deepnote.svg" alt="Launch in Deepnote"/>
  </a>
</div>

        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022_07_19_Deploying_Custom_Models_in_Vertex_AI.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Introduction">Introduction<a class="anchor-link" href="#Introduction"> </a></h2><p>This tutorial is for people who want to deploy a functioning custom PyTorch model in Vertex AI. This tutorial is not a substitute for reading the documentation of Vertex AI, TorchServe, PyTorch, or any other service used in this tutorial. This is only for people who want to learn and get an introduction through hands-on, project-based learning.</p>
<h2 id="Background">Background<a class="anchor-link" href="#Background"> </a></h2><p>Vertex AI has now taken over as Google Cloud Platform's center for all things pertaining to AI. As far as I have seen, there are little to no tutorials going in to deploying custom models in vertex AI. In this tutorial, I am going to define each step to host your own custom PyTorch models in Vertex AI since Vertex AI does not have any prebuilt containers for deploying PyTorch models. You will be able to scale and customize the software around your model as you see fit.</p>
<h2 id="Step-1-Select-Your-Environment">Step 1 Select Your Environment<a class="anchor-link" href="#Step-1-Select-Your-Environment"> </a></h2><p>We will be working with docker images and google cloud SDK. I personally recommend spinning up a jupyter notebook instance in vertex AI workbench, or running your own jupyter instance on your local machine. Docker is incompatible with google colab and other high level notebook services so you will need a dedicated instance that can run docker.</p>
<h2 id="Step-2-Collect-your-Artifacts">Step 2 Collect your Artifacts<a class="anchor-link" href="#Step-2-Collect-your-Artifacts"> </a></h2><p>Now that you have your dedicated work environment, you will need to collect your model artifacts into whatever environment you are working in. First thing is first, let's organize our files. Create a new directory titled 'predictor' and within that directory, another one called, 'model'.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="err">!</span><span class="n">mkdir</span> <span class="n">predictor</span>
<span class="err">!</span><span class="n">mkdir</span> <span class="n">predictor</span><span class="o">/</span><span class="n">model</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Within the model directory, place all of your model artifacts. Your model artifacts are a list of files needed to perform inference on your model. This will range from .bin file storing the weights of your model, to config files, to a tokenizer, all models will be different. But simply put, whatever files you need to perform inference on your model, put them in a predictor/model directory.</p>
<h2 id="Step-3-Preparing-to-Dockerize-TorchServe">Step 3 Preparing to Dockerize TorchServe<a class="anchor-link" href="#Step-3-Preparing-to-Dockerize-TorchServe"> </a></h2><p>To host any model in Vertex AI, your docker image needs to contain instructions to run a HTTP server that can serve as a means of retrieving inferences and checking the health of the server. There are many ways to do this, such as making a flask app along with most major ML frameworks containing some kind of their own serving software. TorchServe is the one for PyTorch models.</p>
<p>TorchServe requires what is called a handler. A handler is a python class that handles all the pre/post processing of inputs/outputs as well as the actual code of loading your model, tokenizer, whatever you need for performing inference on your model. TorchServe has many prebuilt handlers <a href="https://pytorch.org/serve/default_handlers.html">found here</a>. Here is my own custom handler.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">from</span> <span class="nn">os</span> <span class="kn">import</span> <span class="n">listdir</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">OPTForCausalLM</span>
<span class="kn">from</span> <span class="nn">ts.torch_handler.base_handler</span> <span class="kn">import</span> <span class="n">BaseHandler</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">TransformersHandler</span><span class="p">(</span><span class="n">BaseHandler</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This handler takes in a input string and multiple parameters and returns autoregressive generations from various OPT models. </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TransformersHandler</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initialized</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; </span>
<span class="sd">        The function looks at the specs of the device that is running the server and loads in the model and any other objects that must be loaded in.</span>
<span class="sd">        </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># get the passed properties of the torchserve compiler and the device </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">manifest</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">manifest</span>
        <span class="n">properties</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">system_properties</span>
        <span class="n">model_dir</span> <span class="o">=</span> <span class="n">properties</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;model_dir&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">properties</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;gpu_id&quot;</span><span class="p">))</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

        <span class="c1"># Read model serialize/pt file</span>
        <span class="n">serialized_file</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">manifest</span><span class="p">[</span><span class="s2">&quot;model&quot;</span><span class="p">][</span><span class="s2">&quot;serializedFile&quot;</span><span class="p">]</span>
        
        <span class="n">model_pt_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">model_dir</span><span class="p">,</span> <span class="n">serialized_file</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">model_pt_path</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Missing the model.pt or pytorchf_model.bin file&quot;</span><span class="p">)</span>
        
        <span class="c1"># Load model</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Loading Model...&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">OPTForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_dir</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Model loaded...&quot;</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s1">&#39;Transformer model from path </span><span class="si">{0}</span><span class="s1"> loaded successfully&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model_dir</span><span class="p">))</span>
        
        <span class="c1"># Ensure to use the same tokenizer used during training</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Loading tokenizer...&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_dir</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Tokenizer loaded&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">initialized</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The initial entry of data being passed for inference. </span>
<span class="sd">        Here it is where we extract the parameters and inputs. </span>
<span class="sd">        Inputs are tokenized for inference.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;parameters&quot;</span><span class="p">)</span>
        <span class="n">text</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;data&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;text&#39;</span><span class="p">)</span>
        
        <span class="c1"># set the params </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_return_sequences</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;num_return_sequences&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">top_p</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;top_p&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">top_k</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;top_k&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;temperature&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;max_length&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">no_repeat_ngram_size</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;no_repeat_ngram_size&#39;</span><span class="p">)</span>
        
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">inputs</span>

    <span class="k">def</span> <span class="nf">inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function for performing inference on the processed input. The predictions are then decoded and returned.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="n">prediction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>
                                         <span class="n">max_length</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
                                         <span class="n">num_return_sequences</span><span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_return_sequences</span><span class="p">,</span>
                                         <span class="n">do_sample</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                                         <span class="n">temperature</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span><span class="p">,</span>
                                         <span class="n">early_stopping</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                                         <span class="n">top_k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">top_k</span><span class="p">,</span>
                                         <span class="n">top_p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">top_p</span><span class="p">,</span>
                                         <span class="n">no_repeat_ngram_size</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
                                         <span class="n">return_dict_in_generate</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                                         <span class="n">tokenizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">)</span>
        
        
        <span class="n">prediction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">prediction</span><span class="p">[</span><span class="s1">&#39;sequences&#39;</span><span class="p">],</span>
                                                  <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                                  <span class="n">clean_up_tokenization_spaces</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="p">[</span><span class="n">prediction</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">postprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inference_output</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Extra function for processing inference outputs if not already done so.</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">return</span> <span class="n">inference_output</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is a custom handler for inference on an autoregressive language model from huggingface, in this case I am using OPT-125m. You can see that the handler is made up of different pieces such as initializing the model for inference, processing inputs and outputs. Whatever your handler is, it will have to match the specifics of the model you want to perform inference on and what pre/post processing are involved with the inputs/outputs of said model.</p>
<p>Place your handler in the predictor directory.</p>
<h2 id="Step-4-Define-Statics">Step 4 Define Statics<a class="anchor-link" href="#Step-4-Define-Statics"> </a></h2><p>We need to assign some variables that pertain to your specific project. Project ID should be your GCP project id, app name should be whatever you want your app to be called.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">PROJECT_ID</span> <span class="o">=</span> <span class="s1">&#39;my-fun-project&#39;</span>
<span class="n">APP_NAME</span> <span class="o">=</span> <span class="s2">&quot;vertex-test&quot;</span>
<span class="n">CUSTOM_PREDICTOR_IMAGE_URI</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;gcr.io/</span><span class="si">{</span><span class="n">PROJECT_ID</span><span class="si">}</span><span class="s2">/pytorch_predict_</span><span class="si">{</span><span class="n">APP_NAME</span><span class="si">}</span><span class="s2">&quot;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Step-5-Write-Your-Dockerfile">Step 5 Write Your Dockerfile<a class="anchor-link" href="#Step-5-Write-Your-Dockerfile"> </a></h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">docker_file</span> <span class="o">=</span> <span class="s1">&#39;&#39;&#39;   </span>
<span class="s1">bash -s $APP_NAME</span>

<span class="s1">APP_NAME=$1</span>

<span class="s1">cat &lt;&lt; EOF &gt; ./predictor/Dockerfile</span>

<span class="s1">FROM pytorch/torchserve:latest-cpu</span>

<span class="s1"># install dependencies</span>
<span class="s1">RUN python3 -m pip install --upgrade pip</span>
<span class="s1">RUN pip3 install transformers</span>
<span class="s1">RUN pip3 install torch</span>

<span class="s1">USER model-server</span>

<span class="s1"># copy model artifacts, custom handler and other dependencies</span>
<span class="s1">COPY </span><span class="si">{0}</span><span class="s1"> /home/model-server/</span>
<span class="s1">COPY ./model/ /home/model-server/</span>

<span class="s1"># create torchserve configuration files</span>
<span class="s1">USER root</span>
<span class="s1">RUN printf &quot;</span><span class="se">\\</span><span class="s1">nservice_envelope=json&quot; &gt;&gt; /home/model-server/config.properties</span>
<span class="s1">RUN printf &quot;</span><span class="se">\\</span><span class="s1">ninference_address=http://0.0.0.0:7080&quot; &gt;&gt; /home/model-server/config.properties</span>
<span class="s1">RUN printf &quot;</span><span class="se">\\</span><span class="s1">nmanagement_address=http://0.0.0.0:7081&quot; &gt;&gt; /home/model-server/config.properties</span>
<span class="s1">USER model-server</span>

<span class="s1"># expose health and prediction listener ports from the image</span>
<span class="s1">EXPOSE 7080</span>
<span class="s1">EXPOSE 7081</span>

<span class="s1"># create model archive file packaging model artifacts and dependencies</span>
<span class="s1">RUN torch-model-archiver -f   --model-name=</span><span class="si">{1}</span><span class="s1">   --version=</span><span class="si">{7}</span><span class="s1">   --serialized-file=/home/model-server/</span><span class="si">{4}</span><span class="s1">   --handler=/home/model-server/</span><span class="si">{5}</span><span class="s1">   --extra-files &quot;</span><span class="si">{6}</span><span class="s1">&quot;   --export-path=/home/model-server/model-store</span>

<span class="s1"># run Torchserve HTTP serve to respond to prediction requests</span>
<span class="s1">CMD [&quot;torchserve&quot;,      &quot;--start&quot;,      &quot;--ts-config=/home/model-server/config.properties&quot;,      &quot;--models&quot;,      &quot;</span><span class="si">{2}</span><span class="s1">=</span><span class="si">{3}</span><span class="s1">.mar&quot;,      &quot;--model-store&quot;,      &quot;/home/model-server/model-store&quot;]</span>

<span class="s1">EOF</span>

<span class="s1">echo &quot;Writing ./predictor/Dockerfile&quot;</span>
<span class="s1">&#39;&#39;&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">handler_name</span><span class="p">,</span><span class="n">APP_NAME</span><span class="p">,</span><span class="n">APP_NAME</span><span class="p">,</span><span class="n">APP_NAME</span><span class="p">,</span><span class="n">model_source</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[</span><span class="n">model_source</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)],</span><span class="n">handler_name</span><span class="p">,</span><span class="s1">&#39;,&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">extra_files</span><span class="p">),</span><span class="nb">str</span><span class="p">(</span><span class="n">VERSION</span><span class="p">))</span>

<span class="c1">#write docker file</span>
<span class="n">os</span><span class="o">.</span><span class="n">system</span><span class="p">(</span><span class="n">docker_file</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The Dockerfile must be within your predictor directory.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Step-6-Build-and-Push-Image-to-GCP">Step 6 Build and Push Image to GCP<a class="anchor-link" href="#Step-6-Build-and-Push-Image-to-GCP"> </a></h2><p>Now that we have our Dockerfile all set, the next thing to do is the use docker to build and deploy an image in GCP. To do this, we need to determine what our image URI will be in GCP and build the Dockerfile to the image. Docker will take the Dockerfile and the artifacts for our model/software that are all in the predictor directory, and build it into an image. When the image is built, we then want to deploy it into the GCP registry.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">CUSTOM_PREDICTOR_IMAGE_URI</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;gcr.io/</span><span class="si">{</span><span class="n">PROJECT_ID</span><span class="si">}</span><span class="s2">/pytorch_predict_</span><span class="si">{</span><span class="n">APP_NAME</span><span class="si">}</span><span class="s2">&quot;</span>

<span class="c1"># build the image</span>
<span class="err">!</span><span class="n">docker</span> <span class="n">build</span> <span class="o">--</span><span class="n">tag</span><span class="o">=</span><span class="err">$</span><span class="n">CUSTOM_PREDICTOR_IMAGE_URI</span> <span class="o">./</span><span class="n">predictor</span>

<span class="c1"># deploy the image to the GCP registry</span>
<span class="err">!</span><span class="n">docker</span> <span class="n">push</span> <span class="err">$</span><span class="n">CUSTOM_PREDICTOR_IMAGE_URI</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Step-7-Docker-Image-to-Vertex-AI-Model">Step 7 Docker Image to Vertex AI Model<a class="anchor-link" href="#Step-7-Docker-Image-to-Vertex-AI-Model"> </a></h2><p>Normally, docker images are deployed into a container, or a runnable instance of that docker image. To move past Vertex nomenclature, we can view vertex AI endpoints as containers and models as images. We are simply taking an image in the GCP container registry and moving it into Vertex AI as a "model". We will do this with the GCP python SDK.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="kn">from</span> <span class="nn">google.cloud</span> <span class="kn">import</span> <span class="n">aiplatform</span>

<span class="n">aiplatform</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">project</span><span class="o">=</span><span class="n">PROJECT_ID</span><span class="p">)</span>
<span class="n">VERSION</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">model_display_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">APP_NAME</span><span class="si">}</span><span class="s2">-v</span><span class="si">{</span><span class="n">VERSION</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="n">model_description</span> <span class="o">=</span> <span class="s2">&quot;This is so fun&quot;</span>

<span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="n">APP_NAME</span>
<span class="n">health_route</span> <span class="o">=</span> <span class="s2">&quot;/ping&quot;</span>
<span class="n">predict_route</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;/predictions/</span><span class="si">{</span><span class="n">MODEL_NAME</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="n">serving_container_ports</span> <span class="o">=</span> <span class="p">[</span><span class="mi">7080</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">Model</span><span class="o">.</span><span class="n">upload</span><span class="p">(</span>
    <span class="n">display_name</span><span class="o">=</span><span class="n">model_display_name</span><span class="p">,</span>
    <span class="n">description</span><span class="o">=</span><span class="n">model_description</span><span class="p">,</span>
    <span class="n">serving_container_image_uri</span><span class="o">=</span><span class="n">CUSTOM_PREDICTOR_IMAGE_URI</span><span class="p">,</span>
    <span class="n">serving_container_predict_route</span><span class="o">=</span><span class="n">predict_route</span><span class="p">,</span>
    <span class="n">serving_container_health_route</span><span class="o">=</span><span class="n">health_route</span><span class="p">,</span>
    <span class="n">serving_container_ports</span><span class="o">=</span><span class="n">serving_container_ports</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">display_name</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">resource_name</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Step-8-Containerizing-your-Vertex-Model">Step 8 Containerizing your Vertex Model<a class="anchor-link" href="#Step-8-Containerizing-your-Vertex-Model"> </a></h2><p>Now that you have deployed your image/model in vertex AI, you now need to containerize it, or give it a home within a runnable instance. In Vertex AI, these are called endpoints. An endpoint is what assigns the machine to run your callable http server to perform inferences and health checks.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">endpoint_display_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">APP_NAME</span><span class="si">}</span><span class="s2">-endpoint&quot;</span>
<span class="n">endpoint</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">Endpoint</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">display_name</span><span class="o">=</span><span class="n">endpoint_display_name</span><span class="p">)</span>


<span class="n">traffic_percentage</span> <span class="o">=</span> <span class="mi">100</span> <span class="c1"># If you have multiple models assigned to a single endpoint, you can split traffic between the models. </span>
<span class="n">machine_type</span> <span class="o">=</span> <span class="s2">&quot;n1-standard-16&quot;</span> <span class="c1"># assign what type of machine you want in GCP, list found here https://cloud.google.com/vertex-ai/docs/predictions/configure-compute</span>
<span class="n">deployed_model_display_name</span> <span class="o">=</span> <span class="n">model_display_name</span>
<span class="n">sync</span> <span class="o">=</span> <span class="kc">True</span>
<span class="c1"># deploy your model, to your endpoint. This may take some time. </span>
<span class="n">model</span><span class="o">.</span><span class="n">deploy</span><span class="p">(</span>
    <span class="n">endpoint</span><span class="o">=</span><span class="n">endpoint</span><span class="p">,</span>
    <span class="n">deployed_model_display_name</span><span class="o">=</span><span class="n">deployed_model_display_name</span><span class="p">,</span>
    <span class="n">machine_type</span><span class="o">=</span><span class="n">machine_type</span><span class="p">,</span>
    <span class="n">traffic_percentage</span><span class="o">=</span><span class="n">traffic_percentage</span><span class="p">,</span>
    <span class="n">sync</span><span class="o">=</span><span class="n">sync</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>When your model has been deployed to your endpoint, you will get a message of something like "projects/{myprojectID}/locations/us-central1/endpoints/{endpoint ID}". This is what is known as the "resource name" and you will need it to perform inferences.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Step-9-Performing-Inference">Step 9 Performing Inference<a class="anchor-link" href="#Step-9-Performing-Inference"> </a></h2><p>To perform inference, make an endpoint object and pass the endpoint resource name as the only argument. Format json-like data to pass to your endpoint object that coincides with whatever your handler takes in. For my handler, I define variable t as my input for inference.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">endpoint</span> <span class="o">=</span> <span class="n">aiplatform</span><span class="o">.</span><span class="n">Endpoint</span><span class="p">(</span><span class="s1">&#39;{my endpoint resource}&#39;</span><span class="p">)</span>
<span class="n">endpoint</span><span class="o">.</span><span class="n">list_models</span><span class="p">()</span>

<span class="n">t</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">&quot;data&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;This is a test&quot;</span>
        <span class="p">},</span>
        <span class="s2">&quot;parameters&quot;</span><span class="p">:{</span>
            <span class="s2">&quot;num_return_sequences&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
            <span class="s2">&quot;top_p&quot;</span><span class="p">:</span><span class="mf">0.9</span><span class="p">,</span>
            <span class="s2">&quot;top_k&quot;</span><span class="p">:</span><span class="mi">10</span><span class="p">,</span>
            <span class="s2">&quot;temperature&quot;</span><span class="p">:</span><span class="mf">0.8</span><span class="p">,</span>
            <span class="s2">&quot;max_length&quot;</span><span class="p">:</span><span class="mi">20</span><span class="p">,</span>
            <span class="s2">&quot;no_repeat_ngram_size&quot;</span><span class="p">:</span><span class="mi">2</span>
            
            
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">]</span>

<span class="c1"># call the prediction </span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">endpoint</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">instances</span><span class="o">=</span><span class="n">t</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Conclusion">Conclusion<a class="anchor-link" href="#Conclusion"> </a></h2><p>And there you have it, you will have your model's prediction. In this tutorial I walk through how to deploy custom PyTorch models in Vertex AI. To continue your educational journey, play around with what you produced, make it your own, and read up on the many services used here. I hope you found it useful and I am happy to connect with anyone via linkedin.</p>
<p>All the code and the deployment of PyTorch models have been packaged in a public software I wrote which you can find here. 
<a href="https://github.com/jamesonhohbein/vertex_deployment_4dummies">https://github.com/jamesonhohbein/vertex_deployment_4dummies</a></p>
<p>Warm regards,</p>
<p>Jameson</p>
<p><a href="https://www.linkedin.com/in/jameson-hohbein/">Linkedin</a> <a href="https://github.com/jamesonhohbein">Github</a></p>

</div>
</div>
</div>
</div>



  </div><a class="u-url" href="/blog/2022/07/18/_07_19_Deploying_Custom_Models_in_Vertex_AI.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A blog for Call Simulator, a technology company. https://callsimulator.com/</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://www.linkedin.com/in/company%2Fcall-simulator" target="_blank" title="company/call-simulator"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/CallSimulator" target="_blank" title="CallSimulator"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
